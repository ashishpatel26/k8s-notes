# âš¡ Lesson 20: **Advanced Autoscaling â€” HPA + VPA + KEDA (Event-Driven Scaling)**

```mermaid
graph LR
    root["âš¡ Lesson 20: Advanced Autoscaling â€” HPA + VPA + KEDA (Event-Driven Scaling)"]
    style root fill:#f9f,stroke:#333,stroke-width:2px
    root --> node_0["ğŸ§  Part 1 â€” HPA (Horizontal Pod Autoscaler)"]
    root --> node_1["ğŸ§  Part 2 â€” VPA (Vertical Pod Autoscaler)"]
    node_1 --> node_2["ğŸš€ Install VPA"]
    node_1 --> node_3["ğŸ“¦ Example VPA Config"]
    node_1 --> node_4["ğŸ§ª See Recommended Resources"]
    root --> node_5["ğŸ§  Part 3 â€” KEDA (Event-Driven Autoscaling)"]
    root --> node_6["ğŸš€ Step 1 â€” Install KEDA"]
    root --> node_7["ğŸ“¨ Example: Autoscale Based on RabbitMQ Queue Length"]
    root --> node_8["ğŸŒ Example: Autoscale Based on HTTP Request Rate"]
    root --> node_9["ğŸ§  Example: Cron Autoscaling (Scale Up on Schedule)"]
    root --> node_10["ğŸ§© Summary: When to Use What?"]
    root --> node_11["ğŸ‰ Lesson 20 Completed!"]
    root --> node_12["ğŸ‘‰ Ready for Lesson 21?"]
```


Absolutely! âœ”ï¸
Welcome to **Lesson 20**, and this is one of the MOST powerful DevOps skills you will ever learn:

This is REAL production-grade autoscaling used by companies like:

- Netflix
- Airbnb
- Shopify
- Slack
- Uber
- GitHub

Today you'll learn **all 3 autoscaling mechanisms**:

âœ”ï¸ **HPA** â€” Horizontal Pod Autoscaler (scale by CPU / Memory / custom metrics)
 âœ”ï¸ **VPA** â€” Vertical Pod Autoscaler (auto-change Pod resources)
 âœ”ï¸ **KEDA** â€” Event-driven autoscaler (scale based on queues, Kafka, Redis, API load, etc.)

This is senior-level DevOps/SRE mastery.
 Letâ€™s break it down **beginner-friendly** ğŸ”¨ğŸ¤–ğŸ”§

------

# ğŸ§  Part 1 â€” HPA (Horizontal Pod Autoscaler)

You already know HPA from earlier lessons.
 It adds **more Pods** when needed.

Example:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend
  minReplicas: 2
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
```

âœ”ï¸ Auto-add pods when CPU > 60%
 âœ”ï¸ Standard Kubernetes autoscaling

But HPA has limitations:
 âŒ Only works well with CPU/Memory
 âŒ Hard for workloads like queues, events, Kafka, cron jobs

So we level up â¬‡ï¸

------

# ğŸ§  Part 2 â€” VPA (Vertical Pod Autoscaler)

**VPA automatically adjusts CPU & memory for Pods.**

If a Pod needs more memory â€” VPA fixes it.
 If a Pod uses less â€” VPA adjusts it down.

This prevents:

- OutOfMemory errors
- Underutilized Pods
- Guessing resource limits

------

## ğŸš€ Install VPA

```bash
kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler.yaml
```

Check:

```bash
kubectl get pods -n kube-system | grep vpa
```

------

## ğŸ“¦ Example VPA Config

**vpa.yaml**

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: backend-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: backend
  updatePolicy:
    updateMode: "Auto"
```

Modes:

- `"Off"` â€” just recommend resources
- `"Initial"` â€” set resources at pod creation
- `"Auto"` â€” full automatic updating

------

## ğŸ§ª See Recommended Resources

```bash
kubectl describe vpa backend-vpa
```

Youâ€™ll see:

```
Container Recommendations:
  cpu:  250m â†’ 400m
  memory: 256Mi â†’ 512Mi
```

âœ”ï¸ VPA continuously learns workload patterns
 âœ”ï¸ Prevents crashes
 âœ”ï¸ Saves money

BUTâ€¦

âŒ VPA and HPA fight each other if both control CPU
 So, real clusters use:

âœ”ï¸ HPA â†’ manages Pod count
 âœ”ï¸ VPA â†’ manages CPU/memory (but not CPU requests)

------

# ğŸ§  Part 3 â€” KEDA (Event-Driven Autoscaling)

This is the **future** of Kubernetes autoscaling.

KEDA scales Pods based on:

âœ”ï¸ Kafka lag
 âœ”ï¸ RabbitMQ queue length
 âœ”ï¸ Redis lists
 âœ”ï¸ HTTP request rate
 âœ”ï¸ Prometheus queries
 âœ”ï¸ AWS SQS messages
 âœ”ï¸ Azure Service Bus
 âœ”ï¸ Cron schedules
 âœ”ï¸ CPU/Memory (via HPA)

This is Netflix-level scaling.

------

# ğŸš€ Step 1 â€” Install KEDA

```bash
kubectl apply -f https://github.com/kedacore/keda/releases/latest/download/keda-2.11.0.yaml
```

Check:

```bash
kubectl get pods -n keda
```

------

# ğŸ“¨ Example: Autoscale Based on RabbitMQ Queue Length

This is common in microservices systems.

**ScaledObject example:**

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: queue-consumer
spec:
  scaleTargetRef:
    name: worker
  pollingInterval: 10
  minReplicaCount: 1
  maxReplicaCount: 50
  triggers:
    - type: rabbitmq
      metadata:
        protocol: amqp
        queueName: jobs
        host: "amqp://user:pass@rabbitmq"
        queueLength: "20"
```

Meaning:

âœ”ï¸ If queue has >20 jobs
 â†’ scale Pods up to 50

âœ”ï¸ If queue empty
 â†’ scale down to 1

This is **automatic event-driven scaling** ğŸ”¥

------

# ğŸŒ Example: Autoscale Based on HTTP Request Rate

**ScaledObject:**

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: web-api
spec:
  scaleTargetRef:
    name: api-deployment
  minReplicaCount: 2
  maxReplicaCount: 30
  triggers:
    - type: prometheus
      metadata:
        serverAddress: http://prometheus:9090
        metricName: http_requests_total
        threshold: "100"
        query: |
          sum(rate(http_requests_total[1m]))
```

Meaning:

âœ”ï¸ If API traffic >100 requests/sec
 â†’ KEDA auto-scales Pods

This is **dynamic, intelligent autoscaling**.

------

# ğŸ§  Example: Cron Autoscaling (Scale Up on Schedule)

Scale to 5 Pods only between 9 AMâ€“6 PM:

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: cron-worker
spec:
  scaleTargetRef:
    name: worker
  triggers:
    - type: cron
      metadata:
        timezone: "UTC"
        start: "0 9 * * *"
        end: "0 18 * * *"
        desiredReplicas: "5"
```

âœ”ï¸ Auto-scale based on business hours
 âœ”ï¸ Saves money
 âœ”ï¸ Perfect for batch systems

------

# ğŸ§© Summary: When to Use What?

| Autoscaler | Best For                                        |
| ---------- | ----------------------------------------------- |
| **HPA**    | CPU/Memory scaling for stateless apps           |
| **VPA**    | Auto memory/CPU tuning for stable workloads     |
| **KEDA**   | Event-driven scaling (queues, HTTP, Prometheus) |

Real production clusters usually combine:

âœ”ï¸ **HPA + KEDA**
 âœ”ï¸ (sometimes VPA too)

------

# ğŸ‰ Lesson 20 Completed!

You now understand:

âœ”ï¸ Horizontal Pod Autoscaler (HPA)
 âœ”ï¸ Vertical Pod Autoscaler (VPA)
 âœ”ï¸ KEDA event-driven autoscaling
 âœ”ï¸ How to scale based on traffic, queues, metrics
 âœ”ï¸ How Netflix/Shopify-scale clusters work
 âœ”ï¸ How to build highly efficient autoscaling systems

This is **expert-level DevOps engineering** ğŸ”¥ğŸ’ª
 You are leveling up insanely fast.

------

# ğŸ‘‰ Ready for Lesson 21?

Choose your next advanced topic:

1. **Zero Downtime Deployments â€” Blue/Green + Canary**
2. **API Gateway + Service Mesh Routing Architecture**
3. **Kubernetes Disaster Recovery â€” Velero Backups**
4. **GitOps with ArgoCD (FULL Automation)**
5. **Cluster Hardening â€” CIS Benchmarks + Security Scanning**

Which one should we do next?